story_id,epic,title,user_role,description,priority,story_points,dependencies,"acceptance_criteria_1","acceptance_criteria_2","acceptance_criteria_3",technical_tasks,definition_of_done
US-001,"Core Service Architecture & Communication",Establish Backend Windows Service Host,Administrator,"As an Administrator, I want the backend application to run as a reliable Windows Service with robust configuration and logging, so that I can manage and monitor the core system for continuous operation and compliance.",Must Have,5,"","Given the solution is built, when I inspect the project structure, then it follows Clean Architecture principles with Service, Application, Domain, and Infrastructure layers.","Given the application is installed, when I check the Windows Services console, then a service named 'DICOM Service' is present and can be started/stopped.","Given the service is running and a loggable event occurs, when I check the logs, then entries are written to both the rolling file and the Windows Event Log, with any PHI data redacted.","WI-001, WI-002","Code reviewed and merged; Unit tests for PHI redaction pass; Service successfully installs and runs on a test machine; All ACs are met."
US-002,"Core Service Architecture & Communication",Implement a Resilient Asynchronous Messaging Framework,System,"As the System, I need a robust and reliable messaging framework using RabbitMQ, so that different parts of the application can communicate asynchronously, handle failures gracefully with retries and a dead-letter queue, and ensure no critical data is lost.",Must Have,13,US-001,"Given the service is running, when the RabbitMQ broker is temporarily unavailable, then the connection manager automatically retries and re-establishes the connection once the broker is back online.","Given a message needs to be sent for a critical operation, when it is published, then it is marked as persistent and sent to a durable queue.","Given a message consumer fails to process a message after a configured number of retries, when the final failure occurs, then the message is rejected and routed to a dedicated Dead-Letter Queue (DLQ) for manual inspection.","WI-003, WI-004, WI-005","Code reviewed and merged; Integration tests with a Testcontainers RabbitMQ instance validate reconnection, persistence, and DLQ routing; All ACs are met."
US-003,"Core Service Architecture & Communication",Establish a Secure Data Access Layer for PostgreSQL,System,"As the System, I need a secure and structured data access layer using Entity Framework Core, so that application data can be reliably persisted to and retrieved from the PostgreSQL database, with schema changes managed through migrations.",Must Have,8,US-001,"Given the EF Core model is defined, when I run the 'database update' command, then the full database schema is created in PostgreSQL and matches the entity relationship diagram.","Given the application needs to perform a database operation, when it uses a repository, then the data is correctly saved or retrieved without the application layer needing a direct reference to the DbContext.","Given the database connection is configured, when the application connects to PostgreSQL, then the connection must use TLS for encryption.","WI-006, WI-007, WI-008","Code reviewed and merged; EF Core migrations run successfully against a test database; Integration tests confirm repositories can perform CRUD operations; All ACs are met."
US-004,DICOM SCP Functionality,Receive and Queue Incoming DICOM Studies,System,"As the System, I need to listen for and accept incoming DICOM studies via C-STORE, so that I can receive data from external modalities in a standard, high-throughput, and non-blocking manner.",Must Have,8,"US-001, US-002","Given the service is running, when a remote DICOM SCU sends a study via C-STORE, then the service accepts the association and receives all DICOM files successfully.","Upon receiving a DICOM file, when the handler logic is executed, then it extracts the metadata and saves the file to a temporary location without blocking the listener from accepting new connections.","Given a study is received successfully, when the C-STORE operation completes, then a message containing the study's metadata and file path is published to a RabbitMQ queue for later processing.","WI-009, WI-010","Code reviewed and merged; E2E test using a DICOM SCU tool (e.g., dcmtk) successfully sends a study to the running service; RabbitMQ message is verified; All ACs are met."
US-005,DICOM SCP Functionality,Automatically Route Incoming Studies,Administrator,"As an Administrator, I want the system to automatically determine the final storage location for incoming DICOM files based on configurable rules, so that studies are organized logically on the file system without manual intervention.",Should Have,3,US-003,"Given I have configured a rule to route all 'CT' modality studies to 'D:\CT_Studies\', when a CT study is processed, then its files are moved to that specific directory.","Given a set of routing rules in the database, when the auto-routing service is given DICOM metadata, then it correctly returns the destination path of the highest priority matching rule.","Given a study is received that does not match any configured rule, when it is processed, then its files are moved to a pre-defined default storage location.","WI-011","Code reviewed and merged; Unit tests for the routing logic cover all rule matching scenarios (modality, AE title, default); All ACs are met."
US-006,"Asynchronous Job Processing Consumers",Persist Ingested DICOM Study Data,System,"As the System, I need a background worker to process queued DICOM studies, so that metadata is reliably written to the database and files are moved to their permanent, routed location, ensuring data integrity and consistency.",Must Have,8,"US-002, US-003, US-004, US-005","Given a message for a new study is in the RabbitMQ queue, when the persistence consumer processes it, then it creates corresponding records in the Patient, Study, Series, and Image tables in the database.","Given a study is processed by the consumer, when the database transaction is successful, then the associated DICOM files are moved from the temporary location to their final destination as determined by the auto-routing service.","Given the database write or file move operation fails, when the consumer processes the message, then the entire operation is rolled back, the message is not acknowledged, and it is eventually sent to the DLQ after retries.","WI-012","Code reviewed and merged; Integration test validates that a message is consumed and results in correct database entries and file locations; Error scenarios are tested to ensure DLQ routing; All ACs are met."
US-007,"Asynchronous Job Processing Consumers",Generate PDFs Asynchronously,System,"As the System, I need to process PDF generation requests in the background, so that the client application remains responsive and large, complex documents can be created without blocking user interaction.",Must Have,5,"US-002, US-003","Given a PDF generation request message is in the queue, when the PDF consumer processes it, then a PDF file is created at the specified output path.","Given the request specifies PDF/A-3 compliance and AES-256 encryption with a password, when the PDF is generated, then the resulting file meets these specifications.","Given an error occurs during PDF generation (e.g., source file not found), when the consumer processes the message, then the message is rejected and routed to the DLQ after retries.","WI-013","Code reviewed and merged; Integration test confirms a message is consumed and a valid PDF is created; Test verifies PDF/A and encryption compliance; All ACs are met."
US-008,"Asynchronous Job Processing Consumers",Manage a Persistent Print Queue,System,"As the System, I need to handle print jobs via a message queue, so that printing can be performed asynchronously, jobs are not lost if the service restarts, and print failures can be tracked and retried.",Must Have,5,"US-002, US-003","Given a print job message is in the queue, when the print consumer processes it, then the job is sent to the specified printer using the Windows Print API.","Given a print job is processed, when the consumer runs, then it updates the job's status in the database to 'Processing', and then to 'Completed' or 'Failed'.","Given the specified printer is offline or an error occurs, when the consumer attempts to print, then it updates the job status to 'Failed' with an error reason, and the message is sent to the DLQ after retries.","WI-014","Code reviewed and merged; Integration test successfully spools a job to a virtual printer (e.g., 'Microsoft Print to PDF'); Database status updates are verified; All ACs are met."
US-009,"Synchronous Inter-Process Communication (IPC)",Provide Real-time Service Status Checks,System,"As the System, I need to provide a synchronous, low-latency communication channel using Named Pipes, so that the client application can perform real-time status checks of the backend service before submitting jobs.",Should Have,3,US-001,"Given the backend service is running, when a client connects to the designated Named Pipe and sends a 'PING' message, then the service immediately responds with a 'PONG' message.","Given the backend service is not running, when a client attempts to connect to the Named Pipe, then the connection fails within a short timeout period.","Given the Named Pipes server is running, when it is handling a client request, then it does not block other background processes like message consumption or DICOM listening.","WI-015, WI-016","Code reviewed and merged; A test client application or integration test successfully connects and receives a 'PONG' response; Service remains responsive during IPC communication; All ACs are met."
US-010,"System Health, Monitoring & Administration",Monitor and Report System Health,Administrator,"As an Administrator, I want the backend service to continuously monitor its own health and the status of its dependencies (Database, Message Queue, Disk Space), so that I can view a real-time health dashboard and be aware of potential issues.",Must Have,5,US-009,"Given the service is running, when the health probe executes, then it checks connectivity to PostgreSQL and RabbitMQ, and measures the available disk space at the DICOM storage location.","Given the health status is collected, when the client application requests the health status via the Named Pipe, then the service returns a serialized object containing the current status of all monitored components.","Given a component check fails (e.g., database is unreachable), when the health probe runs, then the status for that component is correctly reported as 'Unhealthy' in the returned health data.","WI-017, WI-018","Code reviewed and merged; Integration tests verify health checks for both healthy and unhealthy states (using Testcontainers to stop a service); Named Pipe response for health data is verified; All ACs are met."
US-011,"System Health, Monitoring & Administration",Receive Critical System Alerts,Administrator,"As an Administrator, I want to receive email alerts for critical system events, such as a component failure or messages in the dead-letter queue, so that I can take immediate action to resolve problems.",Should Have,3,"US-002, US-010","Given I have configured my email address for alerts, when a system health check detects a critical failure (e.g., database offline), then an alert email is sent to my address.","Given a message is routed to the Dead-Letter Queue (DLQ), when the system detects the DLQ is not empty, then an alert email is sent with details about the poison message.","Given the SMTP server credentials are provided during installation, when the alerting service is triggered, then it securely retrieves these credentials from the Windows Credential Manager to send the email.","WI-019, WI-022","Code reviewed and merged; Unit tests trigger the alerting service and verify it attempts to send an email with the correct content; Integration with a mock SMTP server confirms email delivery; All ACs are met."
US-012,"System Health, Monitoring & Administration",Enforce Automated Data Retention Policies,Administrator,"As an Administrator, I want the system to automatically purge old studies based on a configurable retention policy, so that storage space is managed efficiently and data lifecycle requirements are met.",Could Have,5,"US-001, US-003","Given the data retention period is set to 7 years, when the daily maintenance task runs, then all studies older than 7 years are marked as deleted in the database and their corresponding files are removed from the file system.","Given the retention task has purged several studies, when I check the system's audit trail, then there is a log entry for each study that was deleted by the automated process.","Given the retention policy is disabled in the configuration, when the daily task runs, then no studies are deleted.","WI-020","Code reviewed and merged; Integration test with test data of varying ages confirms that only data older than the retention period is purged; Audit log creation is verified; All ACs are met."
US-013,"System Health, Monitoring & Administration",Perform Scheduled Data Integrity Checks,Administrator,"As an Administrator, I want the system to periodically verify that all study records in the database have corresponding files on disk, so that I can identify and address data inconsistencies like missing files.",Could Have,3,"US-001, US-003","Given the weekly integrity check is running, when it finds a database record whose file path does not exist on the file system, then it writes a warning to the main log file with the study and image details.","Given all files are present for all database records, when the integrity check completes, then it logs a success message with a summary (e.g., 'Verified 10,500 files, 0 missing').","Given the task is configured to run, when the service is active, then the integrity check is executed automatically on its configured schedule (e.g., weekly).","WI-021","Code reviewed and merged; Integration test with a prepared state (some files intentionally deleted) confirms that discrepancies are correctly logged; A successful run with no missing files is also tested; All ACs are met."
US-014,"Security & Compliance Implementation",Integrate Automated Security Vulnerability Scanning,Developer,"As a Developer, I want the CI pipeline to automatically scan our third-party dependencies for known vulnerabilities on every build, so that we can proactively identify and mitigate security risks before they reach production.",Must Have,3,"","Given the CI pipeline is configured, when a new build is triggered, then an OWASP Dependency-Check scan is executed as part of the pipeline.","Given the scan detects a high-severity vulnerability in a NuGet package, when the pipeline runs, then the build fails and notifies the team.","Given the pipeline completes a scan, when I look at the build artifacts, then a detailed vulnerability report is available for review.","WI-023","CI pipeline YAML/configuration is reviewed and merged; A test branch with a known vulnerable package is created and confirms the build fails as expected; A successful build on the main branch produces a clean report; All ACs are met."